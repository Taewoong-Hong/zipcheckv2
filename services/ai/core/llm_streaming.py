"""
ZipCheck AI - LLM ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“ˆ

ë“€ì–¼ LLM ë³‘ë ¬ ìŠ¤íŠ¸ë¦¬ë° ë¡œì§ (GPT-4o-mini + Claude Sonnet)

ì´ ëª¨ë“ˆì€ ë‘ ê°€ì§€ LLMì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ì—¬:
1. GPT-4o-mini: ë¹ ë¥¸ ì´ˆì•ˆ ìƒì„±
2. Claude Sonnet: ì´ˆì•ˆ ê²€ì¦ ë° ê°œì„ 

Architecture:
- stream_gpt_draft(): GPT-4o-mini ì´ˆì•ˆ ìƒì„±
- stream_claude_validation(): Claude Sonnet ê²€ì¦
- merge_dual_streams(): ë‘ ìŠ¤íŠ¸ë¦¼ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ê³  SSE ì´ë²¤íŠ¸ ë¨¸ì§€
- dual_stream_analysis(): í†µí•© ë˜í¼ í•¨ìˆ˜ (conflicts ê°ì§€ í¬í•¨)
"""
import asyncio
import json
import logging
from typing import AsyncGenerator, Callable, Dict, Any, Tuple, Optional, List
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage
from core.prompts import build_judge_prompt

logger = logging.getLogger(__name__)


# ===========================
# íƒ€ì… ë³€í™˜ í—¬í¼ í•¨ìˆ˜
# ===========================
def ensure_text(content: Any) -> str:
    """
    LangChain ì‘ë‹µì˜ contentë¥¼ ì•ˆì „í•˜ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜
    
    LangChainì´ str ë§ê³  list/fragmentë¡œ ì¤„ ìˆ˜ë„ ìˆìœ¼ë‹ˆ ë°©ì–´ì ìœ¼ë¡œ ì²˜ë¦¬
    
    Args:
        content: LangChain ì‘ë‹µì˜ content (str | list[str | dict[str, Any]])
    
    Returns:
        str: ë³€í™˜ëœ ë¬¸ìì—´
    """
    if isinstance(content, str):
        return content
    if isinstance(content, list):
        # ì˜ˆ: ["ë¬¸ì¥1", "ë¬¸ì¥2"] í˜¹ì€ [{"type": "text", "text": "..."}, ...]
        parts: List[str] = []
        for item in content:
            if isinstance(item, str):
                parts.append(item)
            elif isinstance(item, dict):
                # í…ìŠ¤íŠ¸ í•„ë“œê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
                text = item.get("text") or item.get("content") or ""
                if text:
                    parts.append(str(text))
            else:
                parts.append(str(item))
        return "".join(parts)
    # ê¸°íƒ€ íƒ€ì…ì€ ê·¸ëƒ¥ strë¡œ
    return str(content)


# ===========================
# ê°œë³„ LLM ìŠ¤íŠ¸ë¦¬ë° í•¨ìˆ˜
# ===========================
async def stream_gpt_draft(llm_prompt: str) -> AsyncGenerator[Dict[str, Any], None]:
    """
    GPT-4o-mini ì´ˆì•ˆ ìƒì„± ìŠ¤íŠ¸ë¦¬ë°

    Args:
        llm_prompt: LLMì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ (ë¶„ì„ ì»¨í…ìŠ¤íŠ¸ í¬í•¨)

    Yields:
        Dict with keys:
        - phase: 'draft'
        - model: 'gpt-4o-mini'
        - content: ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¡°ê°
        - total_length: í˜„ì¬ê¹Œì§€ ìƒì„±ëœ ì´ ê¸¸ì´
        - done: ì™„ë£Œ ì—¬ë¶€
        - final_content: (done=Trueì¼ ë•Œ) ìµœì¢… ì „ì²´ í…ìŠ¤íŠ¸
        - error: (ì—ëŸ¬ ì‹œ) ì—ëŸ¬ ë©”ì‹œì§€
    """
    llm_draft = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.3,
        streaming=True
    )
    draft_content = ""
    chunk_count = 0

    try:
        async for chunk in llm_draft.astream([HumanMessage(content=llm_prompt)]):
            if hasattr(chunk, 'content') and chunk.content:
                draft_content += ensure_text(chunk.content)
                chunk_count += 1

                # ì´ë²¤íŠ¸ ì „ì†¡ (phase='draft', model='gpt-4o-mini')
                if chunk_count % 5 == 0:  # ë” ìì£¼ ì—…ë°ì´íŠ¸
                    yield {
                        'phase': 'draft',
                        'model': 'gpt-4o-mini',
                        'content': chunk.content,
                        'total_length': len(draft_content),
                        'done': False
                    }

        # ì™„ë£Œ ì´ë²¤íŠ¸
        yield {
            'phase': 'draft',
            'model': 'gpt-4o-mini',
            'content': '',
            'total_length': len(draft_content),
            'done': True,
            'final_content': draft_content
        }

    except Exception as e:
        logger.error(f"GPT ì´ˆì•ˆ ìƒì„± ì‹¤íŒ¨: {e}")
        yield {
            'phase': 'draft',
            'model': 'gpt-4o-mini',
            'error': str(e),
            'done': True
        }


async def stream_claude_validation(draft_content: str) -> AsyncGenerator[Dict[str, Any], None]:
    """
    Claude Sonnet ê²€ì¦ ìŠ¤íŠ¸ë¦¬ë° (ì´ˆì•ˆ ê¸°ë°˜)

    Args:
        draft_content: GPT-4o-miniê°€ ìƒì„±í•œ ì´ˆì•ˆ í…ìŠ¤íŠ¸

    Yields:
        Dict with keys:
        - phase: 'validation'
        - model: 'claude-3-5-sonnet' or 'claude-3-5-haiku' (fallback)
        - content: ê²€ì¦ í…ìŠ¤íŠ¸ ì¡°ê°
        - total_length: í˜„ì¬ê¹Œì§€ ìƒì„±ëœ ì´ ê¸¸ì´
        - done: ì™„ë£Œ ì—¬ë¶€
        - final_content: (done=Trueì¼ ë•Œ) ìµœì¢… ê²€ì¦ í…ìŠ¤íŠ¸
        - error: (ì—ëŸ¬ ì‹œ) ì—ëŸ¬ ë©”ì‹œì§€

    Note:
        - ì´ˆì•ˆì´ ì¶©ë¶„íˆ ìƒì„±ë  ë•Œê¹Œì§€ 3ì´ˆ ëŒ€ê¸°
        - ì´ˆì•ˆì´ 2000ìë¥¼ ì´ˆê³¼í•˜ë©´ ì• 2000ìë§Œ ì‚¬ìš©
        - Claude Sonnet ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ Haikuë¡œ í´ë°±
    """
    # ì´ˆì•ˆì´ ì¶©ë¶„íˆ ìƒì„±ë  ë•Œê¹Œì§€ ëŒ€ê¸°
    await asyncio.sleep(3)

    judge_prompt = build_judge_prompt(draft_content)

    llm_judge = ChatAnthropic(
        model_name="claude-3-5-sonnet-latest",
        temperature=0.1,
        streaming=True,
        timeout=60,
        stop=None
    )
    validation_content = ""
    chunk_count = 0

    try:
        async for chunk in llm_judge.astream([HumanMessage(content=judge_prompt)]):
            if hasattr(chunk, 'content') and chunk.content:
                validation_content += ensure_text(chunk.content)
                chunk_count += 1

                # ì´ë²¤íŠ¸ ì „ì†¡ (phase='validation', model='claude-3-5-sonnet')
                if chunk_count % 5 == 0:
                    yield {
                        'phase': 'validation',
                        'model': 'claude-3-5-sonnet',
                        'content': chunk.content,
                        'total_length': len(validation_content),
                        'done': False
                    }

        # ì™„ë£Œ ì´ë²¤íŠ¸
        yield {
            'phase': 'validation',
            'model': 'claude-3-5-sonnet',
            'content': '',
            'total_length': len(validation_content),
            'done': True,
            'final_content': validation_content
        }

    except Exception as e:
        msg = str(e)
        # Fallback to Haiku
        if "NotFound" in msg or "not_found_error" in msg or "model:" in msg:
            logger.warning("Claude Sonnet ì‹¤íŒ¨, Haikuë¡œ í´ë°±")

            llm_haiku = ChatAnthropic(
                model_name="claude-3-5-haiku-latest",
                temperature=0.1,
                streaming=True,
                timeout=60,
                stop=None
            )
            validation_content = ""

            async for chunk in llm_haiku.astream([HumanMessage(content=judge_prompt)]):
                if hasattr(chunk, 'content') and chunk.content:
                    validation_content += ensure_text(chunk.content)

                    if chunk_count % 5 == 0:
                        yield {
                            'phase': 'validation',
                            'model': 'claude-3-5-haiku',
                            'content': chunk.content,
                            'total_length': len(validation_content),
                            'done': False
                        }

            yield {
                'phase': 'validation',
                'model': 'claude-3-5-haiku',
                'done': True,
                'final_content': validation_content
            }
        else:
            logger.error(f"Claude ê²€ì¦ ì‹¤íŒ¨: {e}")
            yield {
                'phase': 'validation',
                'model': 'claude-3-5-sonnet',
                'error': str(e),
                'done': True
            }


# ===========================
# ë³‘ë ¬ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
# ===========================
async def merge_dual_streams(
    draft_generator: AsyncGenerator[Dict[str, Any], None],
    validation_generator_factory: Callable[[str], AsyncGenerator[Dict[str, Any], None]],
    step_base: float = 6.0,
    progress_base: float = 0.78
) -> AsyncGenerator[str | Tuple[str, str, Optional[Dict[str, Any]]], None]:
    """
    ë“€ì–¼ LLM ë³‘ë ¬ ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ë¨¸ì§€
    """
    draft_content = ""
    validation_content = ""
    draft_done = False
    validation_done = False
    last_validation_event: Optional[Dict[str, Any]] = None

    draft_gen = draft_generator
    validation_gen = None
    draft_start_time = asyncio.get_event_loop().time()

    while not (draft_done and validation_done):
        try:
            # GPT ì´ˆì•ˆ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
            if not draft_done:
                try:
                    draft_event = await asyncio.wait_for(draft_gen.__anext__(), timeout=0.1)

                    if draft_event.get("done"):
                        draft_done = True
                        draft_content = draft_event.get("final_content", draft_content)

                        message = f"âœ… GPT-4o-mini ì´ˆì•ˆ ì™„ë£Œ ({len(draft_content)}ì)"
                        payload = {
                            "step": step_base + 0.1,
                            "phase": "draft",
                            "model": "gpt-4o-mini",
                            "message": message,
                            "progress": progress_base + 0.04,
                            "draft_length": len(draft_content),
                        }
                        yield f"data: {json.dumps(payload, ensure_ascii=False)}\n\n"

                        # Claude ê²€ì¦ ì‹œì‘
                        if validation_gen is None:
                            validation_gen = validation_generator_factory(draft_content)
                            payload = {
                                "step": step_base + 0.2,
                                "phase": "validation",
                                "message": "ğŸ” Claude Sonnet ê²€ì¦ ì‹œì‘...",
                                "progress": progress_base + 0.05,
                            }
                            yield f"data: {json.dumps(payload, ensure_ascii=False)}\n\n"
                    else:
                        total_length = draft_event.get("total_length", 0)
                        if total_length > 0 and total_length % 100 == 0:
                            message = f"ğŸ“ ì´ˆì•ˆ ìƒì„± ì¤‘... ({total_length}ì)"
                            payload = {
                                "step": step_base + 0.1,
                                "phase": "draft",
                                "model": "gpt-4o-mini",
                                "message": message,
                                "progress": progress_base
                                + min(total_length / 2000, 1) * 0.04,
                                "partial_length": total_length,
                            }
                            yield f"data: {json.dumps(payload, ensure_ascii=False)}\n\n"

                except asyncio.TimeoutError:
                    pass
                except StopAsyncIteration:
                    draft_done = True

            # Claude ê²€ì¦ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
            if validation_gen is not None and not validation_done:
                try:
                    validation_event = await asyncio.wait_for(
                        validation_gen.__anext__(), timeout=0.1
                    )
                    last_validation_event = validation_event

                    if validation_event.get("done"):
                        validation_done = True
                        validation_content = validation_event.get(
                            "final_content", validation_content
                        )

                        model_name = validation_event.get(
                            "model", "claude-3-5-sonnet"
                        )
                        message = f"âœ… {model_name} ê²€ì¦ ì™„ë£Œ ({len(validation_content)}ì)"
                        payload = {
                            "step": step_base + 0.2,
                            "phase": "validation",
                            "model": model_name,
                            "message": message,
                            "progress": progress_base + 0.10,
                            "validation_length": len(validation_content),
                        }
                        yield f"data: {json.dumps(payload, ensure_ascii=False)}\n\n"
                    else:
                        total_length = validation_event.get("total_length", 0)
                        if total_length > 0 and total_length % 100 == 0:
                            model_name = validation_event.get(
                                "model", "claude-3-5-sonnet"
                            )
                            message = f"ğŸ” ê²€ì¦ ì¤‘... ({total_length}ì)"
                            payload = {
                                "step": step_base + 0.2,
                                "phase": "validation",
                                "model": model_name,
                                "message": message,
                                "progress": progress_base
                                + 0.06
                                + min(total_length / 2000, 1) * 0.04,
                                "partial_length": total_length,
                            }
                            yield f"data: {json.dumps(payload, ensure_ascii=False)}\n\n"

                except asyncio.TimeoutError:
                    pass
                except StopAsyncIteration:
                    validation_done = True

            # ë³‘ë ¬ ê²€ì¦ ì‹œì‘ (ì´ˆì•ˆ ì‹œì‘ 3ì´ˆ í›„)
            if validation_gen is None and (
                asyncio.get_event_loop().time() - draft_start_time
            ) > 3:
                if draft_content:
                    validation_gen = validation_generator_factory(draft_content)
                    payload = {
                        "step": step_base + 0.2,
                        "phase": "validation",
                        "message": "ğŸ” Claude Sonnet ê²€ì¦ ì‹œì‘ (ë³‘ë ¬)...",
                        "progress": progress_base + 0.05,
                    }
                    yield f"data: {json.dumps(payload, ensure_ascii=False)}\n\n"

            await asyncio.sleep(0.05)

        except Exception as e:
            logger.error(f"ë³‘ë ¬ ìŠ¤íŠ¸ë¦¬ë° ë£¨í”„ ì˜¤ë¥˜: {e}")
            break

    # ìµœì¢… ê²°ê³¼ë¥¼ tupleë¡œ yield (async generatorëŠ” return ë¶ˆê°€)
    yield (draft_content, validation_content, last_validation_event)  # type: ignore

# ===========================
# í†µí•© ë˜í¼ í•¨ìˆ˜
# ===========================
async def dual_stream_analysis(
    llm_prompt: str,
    step_base: float = 6.0,
    progress_base: float = 0.78
) -> AsyncGenerator[str | Tuple[str, str, float, List[str]], None]:
    """
    ë“€ì–¼ LLM ë³‘ë ¬ ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ (í†µí•© ë˜í¼)

    GPT-4o-mini ì´ˆì•ˆ ìƒì„±ê³¼ Claude Sonnet ê²€ì¦ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ê³ ,
    SSE ì´ë²¤íŠ¸ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤. ìµœì¢…ì ìœ¼ë¡œ conflictsë¥¼ ê°ì§€í•˜ê³ 
    confidenceë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

    Args:
        llm_prompt: LLMì— ì „ë‹¬í•  ë¶„ì„ í”„ë¡¬í”„íŠ¸
        step_base: SSE ì´ë²¤íŠ¸ step ê¸°ì¤€ê°’ (ê¸°ë³¸ê°’: 6.0)
        progress_base: SSE ì´ë²¤íŠ¸ progress ê¸°ì¤€ê°’ (ê¸°ë³¸ê°’: 0.78)

    Yields:
        SSE ì´ë²¤íŠ¸ ë¬¸ìì—´ (data: {...}\n\n)

    Returns:
        ìµœì¢… yield: Tuple[str, str, float, List[str]]
        - draft_content: ì™„ì„±ëœ GPT ì´ˆì•ˆ
        - validation_content: ì™„ì„±ëœ Claude ê²€ì¦
        - confidence: ì‹ ë¢°ë„ (0.0~1.0)
        - conflicts: ë¶ˆì¼ì¹˜ í•­ëª© ë¦¬ìŠ¤íŠ¸

    Example:
        ```python
        async for event in dual_stream_analysis(prompt):
            if isinstance(event, str):
                # SSE ì´ë²¤íŠ¸ ì „ì†¡
                yield event
            else:
                # ìµœì¢… ê²°ê³¼ (tuple)
                draft, validation, confidence, conflicts = event
        ```
    """
    # Step 1: ë³‘ë ¬ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
    draft_gen = stream_gpt_draft(llm_prompt)
    validation_factory = stream_claude_validation

    async for event in merge_dual_streams(
        draft_generator=draft_gen,
        validation_generator_factory=validation_factory,
        step_base=step_base,
        progress_base=progress_base
    ):
        # SSE ì´ë²¤íŠ¸ëŠ” ê·¸ëŒ€ë¡œ ì „ë‹¬
        if isinstance(event, str):
            yield event
        else:
            # ìµœì¢… ê²°ê³¼ (tuple) ì²˜ë¦¬
            draft_content, validation_content, last_validation_event = event

            # Step 2: Conflicts ê°ì§€
            conflicts = []
            if "ìˆ˜ì • í•„ìš”" in validation_content:
                conflicts.append("Claudeê°€ ì´ˆì•ˆì— ìˆ˜ì •ì´ í•„ìš”í•˜ë‹¤ê³  íŒë‹¨í–ˆìŠµë‹ˆë‹¤.")
            if "ì¶”ê°€ í•„ìš”" in validation_content:
                conflicts.append("Claudeê°€ ëˆ„ë½ëœ í•­ëª©ì´ ìˆë‹¤ê³  íŒë‹¨í–ˆìŠµë‹ˆë‹¤.")

            # Step 3: Confidence ê³„ì‚°
            if len(conflicts) == 0:
                confidence = 0.95
            else:
                confidence = 0.75

            # ìµœì¢… ê²°ê³¼ë¥¼ tupleë¡œ yield
            yield (draft_content, validation_content, confidence, conflicts)


# ===========================
# Non-Streaming Wrapper (Background Tasks)
# ===========================
async def simple_llm_analysis(
    llm_prompt: str,
    model: str = "gpt-4o-mini",
    max_retries: int = 3,
    temperature: float = 0.3,
    max_tokens: int = 4096,
    timeout: int = 30
) -> str:
    """
    ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ìš© Non-Streaming LLM ë¶„ì„

    SSE ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë²„í—¤ë“œ ì—†ì´ ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ë¥¼ ìœ„í•œ ê°„ë‹¨í•œ ì¬ì‹œë„ ë¡œì§ì…ë‹ˆë‹¤.
    routesì— ì¤‘ë³µë˜ì–´ ìˆë˜ LLM í˜¸ì¶œ ë¡œì§ì„ ì¤‘ì•™í™”í•©ë‹ˆë‹¤.

    Args:
        llm_prompt: ë¶„ì„í•  LLM í”„ë¡¬í”„íŠ¸
        model: OpenAI ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸ê°’: gpt-4o-mini)
        max_retries: ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ê°’: 3)
        temperature: LLM temperature (ê¸°ë³¸ê°’: 0.3)
        max_tokens: ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸ê°’: 4096)
        timeout: ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ ë‹¨ìœ„, ê¸°ë³¸ê°’: 30)

    Returns:
        ìµœì¢… LLM ì‘ë‹µ ë‚´ìš©

    Raises:
        HTTPException: ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ

    Example:
        ```python
        from core.llm_streaming import simple_llm_analysis

        # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…
        final_answer = await simple_llm_analysis(prompt)
        ```
    """
    from fastapi import HTTPException

    llm = ChatOpenAI(
        model=model,
        temperature=temperature,
        max_retries=0,  # ì¬ì‹œë„ëŠ” ìˆ˜ë™ìœ¼ë¡œ ì²˜ë¦¬
        timeout=timeout
    )
    messages = [HumanMessage(content=llm_prompt)]

    last_err = None
    for attempt in range(1, max_retries + 1):
        try:
            response = llm.invoke(messages)
            final_content = ensure_text(response.content)
            logger.info(f"LLM í•´ì„ ì™„ë£Œ (ì‹œë„ {attempt}): {len(final_content)}ì")
            return final_content
        except Exception as e:
            last_err = e
            logger.warning(f"LLM í˜¸ì¶œ ì‹œë„ {attempt} ì‹¤íŒ¨: {e}")
            if attempt < max_retries:
                # ì§€ìˆ˜ ë°±ì˜¤í”„: 1ì´ˆ, 2ì´ˆ, 3ì´ˆ
                await asyncio.sleep(min(1 * attempt, 3))

    # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
    logger.error(f"LLM í˜¸ì¶œ ì „ì²´ ì‹¤íŒ¨ ({max_retries}íšŒ ì‹œë„): {last_err}")
    raise HTTPException(503, "ë¶„ì„ì´ ì§€ì—°ë©ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
