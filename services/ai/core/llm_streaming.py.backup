"""
ZipCheck AI - LLM ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“ˆ

ë“€ì–¼ LLM ë³‘ë ¬ ìŠ¤íŠ¸ë¦¬ë° ë¡œì§ (GPT-4o-mini + Claude Sonnet)

ì´ ëª¨ë“ˆì€ ë‘ ê°€ì§€ LLMì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ì—¬:
1. GPT-4o-mini: ë¹ ë¥¸ ì´ˆì•ˆ ìƒì„±
2. Claude Sonnet: ì´ˆì•ˆ ê²€ì¦ ë° ê°œì„ 

Architecture:
- stream_gpt_draft(): GPT-4o-mini ì´ˆì•ˆ ìƒì„±
- stream_claude_validation(): Claude Sonnet ê²€ì¦
- merge_dual_streams(): ë‘ ìŠ¤íŠ¸ë¦¼ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ê³  SSE ì´ë²¤íŠ¸ ë¨¸ì§€
- dual_stream_analysis(): í†µí•© ë˜í¼ í•¨ìˆ˜ (conflicts ê°ì§€ í¬í•¨)
"""
import asyncio
import json
import logging
from typing import AsyncGenerator, Callable, Dict, Any, Tuple, Optional, List
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage

logger = logging.getLogger(__name__)


# ===========================
# ê°œë³„ LLM ìŠ¤íŠ¸ë¦¬ë° í•¨ìˆ˜
# ===========================
async def stream_gpt_draft(llm_prompt: str) -> AsyncGenerator[Dict[str, Any], None]:
    """
    GPT-4o-mini ì´ˆì•ˆ ìƒì„± ìŠ¤íŠ¸ë¦¬ë°

    Args:
        llm_prompt: LLMì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ (ë¶„ì„ ì»¨í…ìŠ¤íŠ¸ í¬í•¨)

    Yields:
        Dict with keys:
        - phase: 'draft'
        - model: 'gpt-4o-mini'
        - content: ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¡°ê°
        - total_length: í˜„ì¬ê¹Œì§€ ìƒì„±ëœ ì´ ê¸¸ì´
        - done: ì™„ë£Œ ì—¬ë¶€
        - final_content: (done=Trueì¼ ë•Œ) ìµœì¢… ì „ì²´ í…ìŠ¤íŠ¸
        - error: (ì—ëŸ¬ ì‹œ) ì—ëŸ¬ ë©”ì‹œì§€
    """
    llm_draft = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.3,
        max_tokens=4096,
        streaming=True
    )
    draft_content = ""
    chunk_count = 0

    try:
        async for chunk in llm_draft.astream([HumanMessage(content=llm_prompt)]):
            if hasattr(chunk, 'content') and chunk.content:
                draft_content += chunk.content
                chunk_count += 1

                # ì´ë²¤íŠ¸ ì „ì†¡ (phase='draft', model='gpt-4o-mini')
                if chunk_count % 5 == 0:  # ë” ìì£¼ ì—…ë°ì´íŠ¸
                    yield {
                        'phase': 'draft',
                        'model': 'gpt-4o-mini',
                        'content': chunk.content,
                        'total_length': len(draft_content),
                        'done': False
                    }

        # ì™„ë£Œ ì´ë²¤íŠ¸
        yield {
            'phase': 'draft',
            'model': 'gpt-4o-mini',
            'content': '',
            'total_length': len(draft_content),
            'done': True,
            'final_content': draft_content
        }

    except Exception as e:
        logger.error(f"GPT ì´ˆì•ˆ ìƒì„± ì‹¤íŒ¨: {e}")
        yield {
            'phase': 'draft',
            'model': 'gpt-4o-mini',
            'error': str(e),
            'done': True
        }


async def stream_claude_validation(draft_content: str) -> AsyncGenerator[Dict[str, Any], None]:
    """
    Claude Sonnet ê²€ì¦ ìŠ¤íŠ¸ë¦¬ë° (ì´ˆì•ˆ ê¸°ë°˜)

    Args:
        draft_content: GPT-4o-miniê°€ ìƒì„±í•œ ì´ˆì•ˆ í…ìŠ¤íŠ¸

    Yields:
        Dict with keys:
        - phase: 'validation'
        - model: 'claude-3-5-sonnet' or 'claude-3-5-haiku' (fallback)
        - content: ê²€ì¦ í…ìŠ¤íŠ¸ ì¡°ê°
        - total_length: í˜„ì¬ê¹Œì§€ ìƒì„±ëœ ì´ ê¸¸ì´
        - done: ì™„ë£Œ ì—¬ë¶€
        - final_content: (done=Trueì¼ ë•Œ) ìµœì¢… ê²€ì¦ í…ìŠ¤íŠ¸
        - error: (ì—ëŸ¬ ì‹œ) ì—ëŸ¬ ë©”ì‹œì§€

    Note:
        - ì´ˆì•ˆì´ ì¶©ë¶„íˆ ìƒì„±ë  ë•Œê¹Œì§€ 3ì´ˆ ëŒ€ê¸°
        - ì´ˆì•ˆì´ 2000ìë¥¼ ì´ˆê³¼í•˜ë©´ ì• 2000ìë§Œ ì‚¬ìš©
        - Claude Sonnet ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ Haikuë¡œ í´ë°±
    """
    # ì´ˆì•ˆì´ ì¶©ë¶„íˆ ìƒì„±ë  ë•Œê¹Œì§€ ëŒ€ê¸°
    await asyncio.sleep(3)

    # TODO: Phase 3.2ì—ì„œ ì´ í”„ë¡¬í”„íŠ¸ë¥¼ core/prompts.pyì˜ build_judge_prompt()ë¡œ ì¶”ì¶œ
    judge_prompt = f"""ë„ˆëŠ” ë¶€ë™ì‚° ê³„ì•½ ë¦¬ìŠ¤í¬ ì ê²€ ê²€ì¦ìì´ë‹¤.

ë‹¤ìŒì€ ChatGPTê°€ ìƒì„±í•œ ì´ˆì•ˆì´ë‹¤:

{draft_content[:2000] if len(draft_content) > 2000 else draft_content}

ì´ ì´ˆì•ˆì„ ë‹¤ìŒ ê´€ì ì—ì„œ ê²€ì¦í•˜ë¼:
1. ì‚¬ì‹¤ ê´€ê³„ì˜ ì •í™•ì„±
2. ë²•ë¥ ì  í‘œí˜„ì˜ ì ì ˆì„±
3. ëˆ„ë½ëœ ì¤‘ìš” ë¦¬ìŠ¤í¬
4. ê¶Œì¥ ì¡°ì¹˜ì˜ ì‹¤íš¨ì„±

ê²€ì¦ ê²°ê³¼ë¥¼ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ë¼:

### ê²€ì¦ ê²°ê³¼
- ì •í™•í•œ ë‚´ìš©: ...
- ìˆ˜ì • í•„ìš”: ...
- ì¶”ê°€ í•„ìš”: ...

### ìµœì¢… ê¶Œì¥ì‚¬í•­
..."""

    llm_judge = ChatAnthropic(
        model="claude-3-5-sonnet-latest",
        temperature=0.1,
        max_tokens=4096,
        streaming=True
    )
    validation_content = ""
    chunk_count = 0

    try:
        async for chunk in llm_judge.astream([HumanMessage(content=judge_prompt)]):
            if hasattr(chunk, 'content') and chunk.content:
                validation_content += chunk.content
                chunk_count += 1

                # ì´ë²¤íŠ¸ ì „ì†¡ (phase='validation', model='claude-3-5-sonnet')
                if chunk_count % 5 == 0:
                    yield {
                        'phase': 'validation',
                        'model': 'claude-3-5-sonnet',
                        'content': chunk.content,
                        'total_length': len(validation_content),
                        'done': False
                    }

        # ì™„ë£Œ ì´ë²¤íŠ¸
        yield {
            'phase': 'validation',
            'model': 'claude-3-5-sonnet',
            'content': '',
            'total_length': len(validation_content),
            'done': True,
            'final_content': validation_content
        }

    except Exception as e:
        msg = str(e)
        # Fallback to Haiku
        if "NotFound" in msg or "not_found_error" in msg or "model:" in msg:
            logger.warning("Claude Sonnet ì‹¤íŒ¨, Haikuë¡œ í´ë°±")

            llm_haiku = ChatAnthropic(
                model="claude-3-5-haiku-latest",
                temperature=0.1,
                max_tokens=4096,
                streaming=True
            )
            validation_content = ""

            async for chunk in llm_haiku.astream([HumanMessage(content=judge_prompt)]):
                if hasattr(chunk, 'content') and chunk.content:
                    validation_content += chunk.content

                    if chunk_count % 5 == 0:
                        yield {
                            'phase': 'validation',
                            'model': 'claude-3-5-haiku',
                            'content': chunk.content,
                            'total_length': len(validation_content),
                            'done': False
                        }

            yield {
                'phase': 'validation',
                'model': 'claude-3-5-haiku',
                'done': True,
                'final_content': validation_content
            }
        else:
            logger.error(f"Claude ê²€ì¦ ì‹¤íŒ¨: {e}")
            yield {
                'phase': 'validation',
                'model': 'claude-3-5-sonnet',
                'error': str(e),
                'done': True
            }


# ===========================
# ë³‘ë ¬ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
# ===========================
async def merge_dual_streams(
    draft_generator: AsyncGenerator[Dict[str, Any], None],
    validation_generator_factory: Callable[[str], AsyncGenerator[Dict[str, Any], None]],
    step_base: float = 6.0,
    progress_base: float = 0.78
) -> AsyncGenerator[str, Tuple[str, str, Optional[Dict[str, Any]]]]:
    """
    ë“€ì–¼ LLM ë³‘ë ¬ ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ë¨¸ì§€

    ë‘ ê°œì˜ async generatorë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ê³  SSE ì´ë²¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    GPT-4o-mini ì´ˆì•ˆ ìƒì„±ê³¼ Claude Sonnet ê²€ì¦ì„ ë™ì‹œì— ì²˜ë¦¬í•˜ë©°,
    ê° ë‹¨ê³„ì˜ ì§„í–‰ ìƒí™©ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.

    Args:
        draft_generator: GPT-4o-mini ì´ˆì•ˆ ìƒì„± ì œë„ˆë ˆì´í„°
        validation_generator_factory: Claude ê²€ì¦ ì œë„ˆë ˆì´í„° íŒ©í† ë¦¬
            - í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜: (draft_content: str) -> AsyncGenerator
            - ì´ˆì•ˆì´ ì™„ë£Œë˜ë©´ ì´ íŒ©í† ë¦¬ë¡œ ê²€ì¦ ì œë„ˆë ˆì´í„°ë¥¼ ìƒì„±
        step_base: SSE ì´ë²¤íŠ¸ step ê¸°ì¤€ê°’ (ê¸°ë³¸ê°’: 6.0)
        progress_base: SSE ì´ë²¤íŠ¸ progress ê¸°ì¤€ê°’ (ê¸°ë³¸ê°’: 0.78)

    Yields:
        SSE ì´ë²¤íŠ¸ ë¬¸ìì—´ (data: {...}\n\n)

    Returns:
        ìµœì¢… yield: Tuple[str, str, dict]
        - draft_content: ì™„ì„±ëœ GPT ì´ˆì•ˆ
        - validation_content: ì™„ì„±ëœ Claude ê²€ì¦
        - last_validation_event: ë§ˆì§€ë§‰ ê²€ì¦ ì´ë²¤íŠ¸ (model ì •ë³´ í¬í•¨)

    Algorithm:
        1. GPT ì´ˆì•ˆ ìƒì„± ì‹œì‘ (timeout 0.1së¡œ non-blocking)
        2. GPT ì™„ë£Œ ì‹œ Claude ê²€ì¦ ìë™ ì‹œì‘
        3. ë˜ëŠ” GPT ì‹œì‘ 3ì´ˆ í›„ ìë™ ì‹œì‘ (ë³‘ë ¬ ê²€ì¦)
        4. ë‘ ìŠ¤íŠ¸ë¦¼ì´ ëª¨ë‘ ì™„ë£Œë  ë•Œê¹Œì§€ ì´ë²¤íŠ¸ ì „ì†¡
        5. ìµœì¢… ê²°ê³¼ë¥¼ tupleë¡œ ë°˜í™˜
    """
    import json

    draft_content = ""
    validation_content = ""
    draft_done = False
    validation_done = False
    last_validation_event = None

    draft_gen = draft_generator
    validation_gen = None
    draft_start_time = asyncio.get_event_loop().time()

    while not (draft_done and validation_done):
        try:
            # GPT ì´ˆì•ˆ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
            if not draft_done:
                try:
                    draft_event = await asyncio.wait_for(draft_gen.__anext__(), timeout=0.1)

                    if draft_event.get('done'):
                        draft_done = True
                        draft_content = draft_event.get('final_content', draft_content)

                        yield f"data: {json.dumps({
                            'step': step_base + 0.1,
                            'phase': 'draft',
                            'model': 'gpt-4o-mini',
                            'message': f'âœ… GPT-4o-mini ì´ˆì•ˆ ì™„ë£Œ ({len(draft_content)}ì)',
                            'progress': progress_base + 0.04,
                            'draft_length': len(draft_content)
                        }, ensure_ascii=False)}\n\n"

                        # Claude ê²€ì¦ ì‹œì‘
                        if validation_gen is None:
                            validation_gen = validation_generator_factory(draft_content)
                            yield f"data: {json.dumps({
                                'step': step_base + 0.2,
                                'phase': 'validation',
                                'message': 'ğŸ” Claude Sonnet ê²€ì¦ ì‹œì‘...',
                                'progress': progress_base + 0.05
                            }, ensure_ascii=False)}\n\n"
                    else:
                        total_length = draft_event.get('total_length', 0)
                        if total_length > 0 and total_length % 100 == 0:
                            yield f"data: {json.dumps({
                                'step': step_base + 0.1,
                                'phase': 'draft',
                                'model': 'gpt-4o-mini',
                                'message': f'ğŸ“ ì´ˆì•ˆ ìƒì„± ì¤‘... ({total_length}ì)',
                                'progress': progress_base + min(total_length / 2000, 1) * 0.04,
                                'partial_length': total_length
                            }, ensure_ascii=False)}\n\n"

                except asyncio.TimeoutError:
                    pass
                except StopAsyncIteration:
                    draft_done = True

            # Claude ê²€ì¦ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
            if validation_gen is not None and not validation_done:
                try:
                    validation_event = await asyncio.wait_for(validation_gen.__anext__(), timeout=0.1)
                    last_validation_event = validation_event

                    if validation_event.get('done'):
                        validation_done = True
                        validation_content = validation_event.get('final_content', validation_content)

                        model_name = validation_event.get('model', 'claude-3-5-sonnet')
                        yield f"data: {json.dumps({
                            'step': step_base + 0.2,
                            'phase': 'validation',
                            'model': model_name,
                            'message': f'âœ… {model_name} ê²€ì¦ ì™„ë£Œ ({len(validation_content)}ì)',
                            'progress': progress_base + 0.10,
                            'validation_length': len(validation_content)
                        }, ensure_ascii=False)}\n\n"
                    else:
                        total_length = validation_event.get('total_length', 0)
                        if total_length > 0 and total_length % 100 == 0:
                            yield f"data: {json.dumps({
                                'step': step_base + 0.2,
                                'phase': 'validation',
                                'model': validation_event.get('model', 'claude-3-5-sonnet'),
                                'message': f'ğŸ” ê²€ì¦ ì¤‘... ({total_length}ì)',
                                'progress': progress_base + 0.06 + min(total_length / 2000, 1) * 0.04,
                                'partial_length': total_length
                            }, ensure_ascii=False)}\n\n"

                except asyncio.TimeoutError:
                    pass
                except StopAsyncIteration:
                    validation_done = True

            # ë³‘ë ¬ ê²€ì¦ ì‹œì‘ (ì´ˆì•ˆ ì‹œì‘ 3ì´ˆ í›„)
            if validation_gen is None and (asyncio.get_event_loop().time() - draft_start_time) > 3:
                if draft_content:
                    validation_gen = validation_generator_factory(draft_content)
                    yield f"data: {json.dumps({
                        'step': step_base + 0.2,
                        'phase': 'validation',
                        'message': 'ğŸ” Claude Sonnet ê²€ì¦ ì‹œì‘ (ë³‘ë ¬)...',
                        'progress': progress_base + 0.05
                    }, ensure_ascii=False)}\n\n"

            await asyncio.sleep(0.05)

        except Exception as e:
            logger.error(f"ë³‘ë ¬ ìŠ¤íŠ¸ë¦¬ë° ë£¨í”„ ì˜¤ë¥˜: {e}")
            break

    # ìµœì¢… ê²°ê³¼ë¥¼ tupleë¡œ yield (async generatorëŠ” return ë¶ˆê°€)
    yield (draft_content, validation_content, last_validation_event)


# ===========================
# í†µí•© ë˜í¼ í•¨ìˆ˜
# ===========================
async def dual_stream_analysis(
    llm_prompt: str,
    step_base: float = 6.0,
    progress_base: float = 0.78
) -> AsyncGenerator[str, Tuple[str, str, float, List[str]]]:
    """
    ë“€ì–¼ LLM ë³‘ë ¬ ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ (í†µí•© ë˜í¼)

    GPT-4o-mini ì´ˆì•ˆ ìƒì„±ê³¼ Claude Sonnet ê²€ì¦ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ê³ ,
    SSE ì´ë²¤íŠ¸ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤. ìµœì¢…ì ìœ¼ë¡œ conflictsë¥¼ ê°ì§€í•˜ê³ 
    confidenceë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

    Args:
        llm_prompt: LLMì— ì „ë‹¬í•  ë¶„ì„ í”„ë¡¬í”„íŠ¸
        step_base: SSE ì´ë²¤íŠ¸ step ê¸°ì¤€ê°’ (ê¸°ë³¸ê°’: 6.0)
        progress_base: SSE ì´ë²¤íŠ¸ progress ê¸°ì¤€ê°’ (ê¸°ë³¸ê°’: 0.78)

    Yields:
        SSE ì´ë²¤íŠ¸ ë¬¸ìì—´ (data: {...}\n\n)

    Returns:
        ìµœì¢… yield: Tuple[str, str, float, List[str]]
        - draft_content: ì™„ì„±ëœ GPT ì´ˆì•ˆ
        - validation_content: ì™„ì„±ëœ Claude ê²€ì¦
        - confidence: ì‹ ë¢°ë„ (0.0~1.0)
        - conflicts: ë¶ˆì¼ì¹˜ í•­ëª© ë¦¬ìŠ¤íŠ¸

    Example:
        ```python
        async for event in dual_stream_analysis(prompt):
            if isinstance(event, str):
                # SSE ì´ë²¤íŠ¸ ì „ì†¡
                yield event
            else:
                # ìµœì¢… ê²°ê³¼ (tuple)
                draft, validation, confidence, conflicts = event
        ```
    """
    # Step 1: ë³‘ë ¬ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
    draft_gen = stream_gpt_draft(llm_prompt)
    validation_factory = stream_claude_validation

    async for event in merge_dual_streams(
        draft_generator=draft_gen,
        validation_generator_factory=validation_factory,
        step_base=step_base,
        progress_base=progress_base
    ):
        # SSE ì´ë²¤íŠ¸ëŠ” ê·¸ëŒ€ë¡œ ì „ë‹¬
        if isinstance(event, str):
            yield event
        else:
            # ìµœì¢… ê²°ê³¼ (tuple) ì²˜ë¦¬
            draft_content, validation_content, last_validation_event = event

            # Step 2: Conflicts ê°ì§€
            conflicts = []
            if "ìˆ˜ì • í•„ìš”" in validation_content:
                conflicts.append("Claudeê°€ ì´ˆì•ˆì— ìˆ˜ì •ì´ í•„ìš”í•˜ë‹¤ê³  íŒë‹¨í–ˆìŠµë‹ˆë‹¤.")
            if "ì¶”ê°€ í•„ìš”" in validation_content:
                conflicts.append("Claudeê°€ ëˆ„ë½ëœ í•­ëª©ì´ ìˆë‹¤ê³  íŒë‹¨í–ˆìŠµë‹ˆë‹¤.")

            # Step 3: Confidence ê³„ì‚°
            if len(conflicts) == 0:
                confidence = 0.95
            else:
                confidence = 0.75

            # ìµœì¢… ê²°ê³¼ë¥¼ tupleë¡œ yield
            yield (draft_content, validation_content, confidence, conflicts)
